{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('data/csi300.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>code</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>to</th>\n",
       "      <th>vol</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.018292</td>\n",
       "      <td>0.018760</td>\n",
       "      <td>0.018052</td>\n",
       "      <td>0.162606</td>\n",
       "      <td>0.223802</td>\n",
       "      <td>-0.003756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>000002.SZ</td>\n",
       "      <td>0.024635</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.024638</td>\n",
       "      <td>0.024083</td>\n",
       "      <td>0.181035</td>\n",
       "      <td>0.193991</td>\n",
       "      <td>0.003601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>000063.SZ</td>\n",
       "      <td>0.041155</td>\n",
       "      <td>0.040791</td>\n",
       "      <td>0.042311</td>\n",
       "      <td>0.040859</td>\n",
       "      <td>0.070436</td>\n",
       "      <td>0.045882</td>\n",
       "      <td>0.015908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-11-01</td>\n",
       "      <td>000069.SZ</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.006262</td>\n",
       "      <td>0.006240</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.021371</td>\n",
       "      <td>0.073221</td>\n",
       "      <td>0.020572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-11-02</td>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>0.018423</td>\n",
       "      <td>0.018292</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.018497</td>\n",
       "      <td>0.153478</td>\n",
       "      <td>0.211124</td>\n",
       "      <td>-0.018036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>000069.SZ</td>\n",
       "      <td>0.008135</td>\n",
       "      <td>0.008394</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.119776</td>\n",
       "      <td>0.009425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>000001.SZ</td>\n",
       "      <td>0.023289</td>\n",
       "      <td>0.023241</td>\n",
       "      <td>0.023994</td>\n",
       "      <td>0.023193</td>\n",
       "      <td>0.119720</td>\n",
       "      <td>0.132547</td>\n",
       "      <td>0.011335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>000002.SZ</td>\n",
       "      <td>0.032981</td>\n",
       "      <td>0.032835</td>\n",
       "      <td>0.033998</td>\n",
       "      <td>0.033225</td>\n",
       "      <td>0.104168</td>\n",
       "      <td>0.083424</td>\n",
       "      <td>0.076047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>000063.SZ</td>\n",
       "      <td>0.047712</td>\n",
       "      <td>0.047453</td>\n",
       "      <td>0.049337</td>\n",
       "      <td>0.047508</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.036075</td>\n",
       "      <td>0.023027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>000069.SZ</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>0.080766</td>\n",
       "      <td>0.027946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             dt       code     close      high       low      open        to  \\\n",
       "0    2022-11-01  000001.SZ  0.018500  0.018292  0.018760  0.018052  0.162606   \n",
       "1    2022-11-01  000002.SZ  0.024635  0.024345  0.024638  0.024083  0.181035   \n",
       "2    2022-11-01  000063.SZ  0.041155  0.040791  0.042311  0.040859  0.070436   \n",
       "3    2022-11-01  000069.SZ  0.006308  0.006262  0.006240  0.006088  0.021371   \n",
       "4    2022-11-02  000001.SZ  0.018423  0.018292  0.019002  0.018497  0.153478   \n",
       "..          ...        ...       ...       ...       ...       ...       ...   \n",
       "171  2022-12-29  000069.SZ  0.008135  0.008394  0.008495  0.008504  0.043796   \n",
       "172  2022-12-30  000001.SZ  0.023289  0.023241  0.023994  0.023193  0.119720   \n",
       "173  2022-12-30  000002.SZ  0.032981  0.032835  0.033998  0.033225  0.104168   \n",
       "174  2022-12-30  000063.SZ  0.047712  0.047453  0.049337  0.047508  0.064096   \n",
       "175  2022-12-30  000069.SZ  0.008231  0.008185  0.008474  0.008214  0.029514   \n",
       "\n",
       "          vol     label  \n",
       "0    0.223802 -0.003756  \n",
       "1    0.193991  0.003601  \n",
       "2    0.045882  0.015908  \n",
       "3    0.073221  0.020572  \n",
       "4    0.211124 -0.018036  \n",
       "..        ...       ...  \n",
       "171  0.119776  0.009425  \n",
       "172  0.132547  0.011335  \n",
       "173  0.083424  0.076047  \n",
       "174  0.036075  0.023027  \n",
       "175  0.080766  0.027946  \n",
       "\n",
       "[176 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssss\n"
     ]
    }
   ],
   "source": [
    "print('ssss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_pickle('data/valid_csi300.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.reset_index(inplace=True)\n",
    "df_.rename(columns={'datetime': 'dt'}, inplace=True)\n",
    "df_.rename(columns = {'instrument':'code'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = df_[['dt', 'code', 'KMID','KLEN',\t'KMID2','KUP','KUP2','LABEL0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>code</th>\n",
       "      <th>KMID</th>\n",
       "      <th>KLEN</th>\n",
       "      <th>KMID2</th>\n",
       "      <th>KUP</th>\n",
       "      <th>KUP2</th>\n",
       "      <th>LABEL0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600000</td>\n",
       "      <td>0.233334</td>\n",
       "      <td>-1.284120</td>\n",
       "      <td>0.758802</td>\n",
       "      <td>-0.747315</td>\n",
       "      <td>-0.216976</td>\n",
       "      <td>-1.067304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600008</td>\n",
       "      <td>-1.456682</td>\n",
       "      <td>0.149920</td>\n",
       "      <td>-1.233051</td>\n",
       "      <td>-0.404646</td>\n",
       "      <td>-0.542506</td>\n",
       "      <td>0.209689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600009</td>\n",
       "      <td>-0.156256</td>\n",
       "      <td>-0.636687</td>\n",
       "      <td>-0.222577</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>0.651043</td>\n",
       "      <td>-2.116940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600010</td>\n",
       "      <td>0.456733</td>\n",
       "      <td>-0.307101</td>\n",
       "      <td>0.505868</td>\n",
       "      <td>0.204933</td>\n",
       "      <td>0.361681</td>\n",
       "      <td>-0.131998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600015</td>\n",
       "      <td>0.129305</td>\n",
       "      <td>-1.089662</td>\n",
       "      <td>0.303532</td>\n",
       "      <td>-0.144860</td>\n",
       "      <td>1.084999</td>\n",
       "      <td>-0.750083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76438</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300251</td>\n",
       "      <td>-2.353814</td>\n",
       "      <td>2.398624</td>\n",
       "      <td>-0.922465</td>\n",
       "      <td>1.793636</td>\n",
       "      <td>0.106381</td>\n",
       "      <td>1.418828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76439</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300296</td>\n",
       "      <td>0.551733</td>\n",
       "      <td>-0.254601</td>\n",
       "      <td>0.590176</td>\n",
       "      <td>0.453886</td>\n",
       "      <td>0.602806</td>\n",
       "      <td>-2.755846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76440</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300308</td>\n",
       "      <td>-1.384341</td>\n",
       "      <td>1.314281</td>\n",
       "      <td>-0.732111</td>\n",
       "      <td>-0.236152</td>\n",
       "      <td>-0.648825</td>\n",
       "      <td>-2.075338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76441</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300408</td>\n",
       "      <td>-0.317776</td>\n",
       "      <td>-0.605141</td>\n",
       "      <td>-0.440589</td>\n",
       "      <td>-0.806837</td>\n",
       "      <td>-0.804999</td>\n",
       "      <td>-3.124002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76442</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300433</td>\n",
       "      <td>-2.391778</td>\n",
       "      <td>1.276520</td>\n",
       "      <td>-1.280476</td>\n",
       "      <td>-0.527629</td>\n",
       "      <td>-0.813757</td>\n",
       "      <td>-0.392808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt      code      KMID      KLEN     KMID2       KUP      KUP2  \\\n",
       "0     2017-12-04  SH600000  0.233334 -1.284120  0.758802 -0.747315 -0.216976   \n",
       "1     2017-12-04  SH600008 -1.456682  0.149920 -1.233051 -0.404646 -0.542506   \n",
       "2     2017-12-04  SH600009 -0.156256 -0.636687 -0.222577  0.124836  0.651043   \n",
       "3     2017-12-04  SH600010  0.456733 -0.307101  0.505868  0.204933  0.361681   \n",
       "4     2017-12-04  SH600015  0.129305 -1.089662  0.303532 -0.144860  1.084999   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "76438 2018-12-28  SZ300251 -2.353814  2.398624 -0.922465  1.793636  0.106381   \n",
       "76439 2018-12-28  SZ300296  0.551733 -0.254601  0.590176  0.453886  0.602806   \n",
       "76440 2018-12-28  SZ300308 -1.384341  1.314281 -0.732111 -0.236152 -0.648825   \n",
       "76441 2018-12-28  SZ300408 -0.317776 -0.605141 -0.440589 -0.806837 -0.804999   \n",
       "76442 2018-12-28  SZ300433 -2.391778  1.276520 -1.280476 -0.527629 -0.813757   \n",
       "\n",
       "         LABEL0  \n",
       "0     -1.067304  \n",
       "1      0.209689  \n",
       "2     -2.116940  \n",
       "3     -0.131998  \n",
       "4     -0.750083  \n",
       "...         ...  \n",
       "76438  1.418828  \n",
       "76439 -2.755846  \n",
       "76440 -2.075338  \n",
       "76441 -3.124002  \n",
       "76442 -0.392808  \n",
       "\n",
       "[76443 rows x 8 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "feature_cols = ['KMID','KLEN','KMID2','KUP','KUP2']\n",
    "\n",
    "def cal_pccs(x, y, n):\n",
    "    sum_xy = np.sum(np.sum(x*y)) #요소간 곱한 것을 모두 더하기 위함임. e.g. [1,2,3]*[1,2,3] -> 근데 굳이 이렇게 해야하나...\n",
    "    sum_x = np.sum(np.sum(x))\n",
    "    sum_y = np.sum(np.sum(y))\n",
    "    sum_x2 = np.sum(np.sum(x*x))\n",
    "    sum_y2 = np.sum(np.sum(y*y))\n",
    "    pcc = (n*sum_xy-sum_x*sum_y)/np.sqrt((n*sum_x2-sum_x*sum_x)*(n*sum_y2-sum_y*sum_y))\n",
    "    return pcc\n",
    "\n",
    "def calculate_pccs(xs, yss, n): # ref_dict[code], ref_dict, n\n",
    "    '''\n",
    "    각 stock code의 각 feature 별로 상관관계가 계산된다. 이 때 tmp_res에는 각 주식 code별 상관관계가 저장되며 임시 tmp_res는 results에 저장된다.\n",
    "     '''\n",
    "    result = []\n",
    "    for name in yss:\n",
    "        ys = yss[name]\n",
    "        tmp_res = []\n",
    "        for pos, x in enumerate(xs): # pos는 feature의 index, x는 feature의 20일 동안의 데이터\n",
    "            y = ys[pos] # 하나의 feature에 대해서 20일 어치의 데이터를 가져온다.\n",
    "            tmp_res.append(cal_pccs(x, y, n)) # x: 주식 code, y: features, n: 참조할 과거 시계열 길이 -> pearson 상관관계 값을 구한다.\n",
    "        result.append(tmp_res) # 각code 별로 진행.\n",
    "    return np.mean(result, axis=1) # mean of all pccs -> 6개의 feature에 대한 평균 pccs -> 이렇게 하면 안되지 않니... 어떻게 했지 \n",
    "\n",
    "def stock_cor_matrix(ref_dict, codes, n, processes=1):\n",
    "    if processes > 1:\n",
    "        pool = mp.Pool(processes=processes)\n",
    "        args_all = [(ref_dict[code], ref_dict, n) for code in codes] # 전체 코드에 대해서 각각의 코드에 대한 pccs를 구하고 튜플로서 저장\n",
    "        results = [pool.apply_async(calculate_pccs, args=args) for args in args_all] # 저장된 튜플(x,y,n)이 있으면 그것에 대해서 멀\n",
    "        output = [o.get() for o in results]\n",
    "        data = np.stack(output) #  하나의 배열로서 합친다.\n",
    "        return pd.DataFrame(data=data, index=codes, columns=codes)\n",
    "    data = np.zeros([len(codes), len(codes)])\n",
    "    for i in tqdm(range(len(codes))): # 행은 code, 열은 feature\n",
    "        data[i, :] = calculate_pccs(ref_dict[codes[i]], ref_dict, n)\n",
    "    return pd.DataFrame(data=data, index=codes, columns=codes)\n",
    "\n",
    "path1 = \"./data/valid_csi300.pkl\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>code</th>\n",
       "      <th>KMID</th>\n",
       "      <th>KLEN</th>\n",
       "      <th>KMID2</th>\n",
       "      <th>KUP</th>\n",
       "      <th>KUP2</th>\n",
       "      <th>LABEL0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600000</td>\n",
       "      <td>0.233334</td>\n",
       "      <td>-1.284120</td>\n",
       "      <td>0.758802</td>\n",
       "      <td>-0.747315</td>\n",
       "      <td>-0.216976</td>\n",
       "      <td>-1.067304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600008</td>\n",
       "      <td>-1.456682</td>\n",
       "      <td>0.149920</td>\n",
       "      <td>-1.233051</td>\n",
       "      <td>-0.404646</td>\n",
       "      <td>-0.542506</td>\n",
       "      <td>0.209689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600009</td>\n",
       "      <td>-0.156256</td>\n",
       "      <td>-0.636687</td>\n",
       "      <td>-0.222577</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>0.651043</td>\n",
       "      <td>-2.116940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600010</td>\n",
       "      <td>0.456733</td>\n",
       "      <td>-0.307101</td>\n",
       "      <td>0.505868</td>\n",
       "      <td>0.204933</td>\n",
       "      <td>0.361681</td>\n",
       "      <td>-0.131998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-04</td>\n",
       "      <td>SH600015</td>\n",
       "      <td>0.129305</td>\n",
       "      <td>-1.089662</td>\n",
       "      <td>0.303532</td>\n",
       "      <td>-0.144860</td>\n",
       "      <td>1.084999</td>\n",
       "      <td>-0.750083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76438</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300251</td>\n",
       "      <td>-2.353814</td>\n",
       "      <td>2.398624</td>\n",
       "      <td>-0.922465</td>\n",
       "      <td>1.793636</td>\n",
       "      <td>0.106381</td>\n",
       "      <td>1.418828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76439</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300296</td>\n",
       "      <td>0.551733</td>\n",
       "      <td>-0.254601</td>\n",
       "      <td>0.590176</td>\n",
       "      <td>0.453886</td>\n",
       "      <td>0.602806</td>\n",
       "      <td>-2.755846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76440</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300308</td>\n",
       "      <td>-1.384341</td>\n",
       "      <td>1.314281</td>\n",
       "      <td>-0.732111</td>\n",
       "      <td>-0.236152</td>\n",
       "      <td>-0.648825</td>\n",
       "      <td>-2.075338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76441</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300408</td>\n",
       "      <td>-0.317776</td>\n",
       "      <td>-0.605141</td>\n",
       "      <td>-0.440589</td>\n",
       "      <td>-0.806837</td>\n",
       "      <td>-0.804999</td>\n",
       "      <td>-3.124002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76442</th>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>SZ300433</td>\n",
       "      <td>-2.391778</td>\n",
       "      <td>1.276520</td>\n",
       "      <td>-1.280476</td>\n",
       "      <td>-0.527629</td>\n",
       "      <td>-0.813757</td>\n",
       "      <td>-0.392808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76443 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt      code      KMID      KLEN     KMID2       KUP      KUP2  \\\n",
       "0     2017-12-04  SH600000  0.233334 -1.284120  0.758802 -0.747315 -0.216976   \n",
       "1     2017-12-04  SH600008 -1.456682  0.149920 -1.233051 -0.404646 -0.542506   \n",
       "2     2017-12-04  SH600009 -0.156256 -0.636687 -0.222577  0.124836  0.651043   \n",
       "3     2017-12-04  SH600010  0.456733 -0.307101  0.505868  0.204933  0.361681   \n",
       "4     2017-12-04  SH600015  0.129305 -1.089662  0.303532 -0.144860  1.084999   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "76438 2018-12-28  SZ300251 -2.353814  2.398624 -0.922465  1.793636  0.106381   \n",
       "76439 2018-12-28  SZ300296  0.551733 -0.254601  0.590176  0.453886  0.602806   \n",
       "76440 2018-12-28  SZ300308 -1.384341  1.314281 -0.732111 -0.236152 -0.648825   \n",
       "76441 2018-12-28  SZ300408 -0.317776 -0.605141 -0.440589 -0.806837 -0.804999   \n",
       "76442 2018-12-28  SZ300433 -2.391778  1.276520 -1.280476 -0.527629 -0.813757   \n",
       "\n",
       "         LABEL0  \n",
       "0     -1.067304  \n",
       "1      0.209689  \n",
       "2     -2.116940  \n",
       "3     -0.131998  \n",
       "4     -0.750083  \n",
       "...         ...  \n",
       "76438  1.418828  \n",
       "76439 -2.755846  \n",
       "76440 -2.075338  \n",
       "76441 -3.124002  \n",
       "76442 -0.392808  \n",
       "\n",
       "[76443 rows x 8 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#df1 = pickle.load(open(path1, 'rb'), encoding='utf-8')\n",
    "#prev_date_num Indicates the number of days in which stock correlation is calculated\n",
    "prev_date_num = 20\n",
    "date_unique=df_['dt'].unique()\n",
    "stock_trade_data=date_unique.tolist()\n",
    "stock_trade_data.sort()\n",
    "stock_num=df_.code.unique().shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "368"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatetimeArray>\n",
       "['2018-01-02 00:00:00', '2018-01-03 00:00:00', '2018-01-04 00:00:00',\n",
       " '2018-01-05 00:00:00', '2018-01-08 00:00:00', '2018-01-09 00:00:00',\n",
       " '2018-01-10 00:00:00', '2018-01-11 00:00:00', '2018-01-12 00:00:00',\n",
       " '2018-01-15 00:00:00',\n",
       " ...\n",
       " '2018-12-17 00:00:00', '2018-12-18 00:00:00', '2018-12-19 00:00:00',\n",
       " '2018-12-20 00:00:00', '2018-12-21 00:00:00', '2018-12-24 00:00:00',\n",
       " '2018-12-25 00:00:00', '2018-12-26 00:00:00', '2018-12-27 00:00:00',\n",
       " '2018-12-28 00:00:00']\n",
       "Length: 243, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_['dt'].unique()[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 리스트를 pandas DataFrame으로 변환\n",
    "df = pd.DataFrame(stock_trade_data, columns=['date'])\n",
    "\n",
    "# 문자열을 datetime 형식으로 변환\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 각 달의 마지막 날짜 추출\n",
    "last_days = df.groupby(df['date'].dt.to_period('M')).max()\n",
    "\n",
    "# DataFrame 형식으로 날짜를 추출\n",
    "last_days_list = last_days['date'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312/312 [00:00<00:00, 1669.59it/s]\n",
      "100%|██████████| 257/257 [00:10<00:00, 23.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257, 257)\n",
      "0000000000\n",
      "368\n",
      "time cost 10.999049663543701 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [00:00<00:00, 1738.09it/s]\n",
      "100%|██████████| 273/273 [00:12<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 273)\n",
      "0000000000\n",
      "368\n",
      "time cost 12.351292371749878 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 291/291 [00:00<00:00, 1774.38it/s]\n",
      "100%|██████████| 278/278 [00:12<00:00, 21.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278, 278)\n",
      "0000000000\n",
      "368\n",
      "time cost 12.884285688400269 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 290/290 [00:00<00:00, 1757.58it/s]\n",
      "100%|██████████| 283/283 [00:13<00:00, 21.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(283, 283)\n",
      "0000000000\n",
      "368\n",
      "time cost 13.361332893371582 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [00:00<00:00, 1817.62it/s]\n",
      "100%|██████████| 280/280 [00:13<00:00, 21.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280, 280)\n",
      "0000000000\n",
      "368\n",
      "time cost 13.053253173828125 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:00<00:00, 1766.87it/s]\n",
      "100%|██████████| 277/277 [00:12<00:00, 21.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 277)\n",
      "0000000000\n",
      "368\n",
      "time cost 12.797405242919922 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 1765.37it/s]\n",
      "100%|██████████| 255/255 [00:10<00:00, 23.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(255, 255)\n",
      "0000000000\n",
      "368\n",
      "time cost 10.846304178237915 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 294/294 [00:00<00:00, 1792.68it/s]\n",
      "100%|██████████| 288/288 [00:13<00:00, 20.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 288)\n",
      "0000000000\n",
      "368\n",
      "time cost 13.85266661643982 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 1732.56it/s]\n",
      "100%|██████████| 291/291 [00:14<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 291)\n",
      "0000000000\n",
      "368\n",
      "time cost 14.031027555465698 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 298/298 [00:00<00:00, 1773.80it/s]\n",
      "100%|██████████| 294/294 [00:14<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(294, 294)\n",
      "0000000000\n",
      "368\n",
      "time cost 14.339455842971802 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 296/296 [00:00<00:00, 1772.45it/s]\n",
      "100%|██████████| 264/264 [00:11<00:00, 22.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(264, 264)\n",
      "0000000000\n",
      "368\n",
      "time cost 11.535927057266235 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:00<00:00, 1738.39it/s]\n",
      "100%|██████████| 293/293 [00:14<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(293, 293)\n",
      "0000000000\n",
      "368\n",
      "time cost 14.230388641357422 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 323/323 [00:00<00:00, 1745.94it/s]\n",
      "100%|██████████| 267/267 [00:11<00:00, 22.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267, 267)\n",
      "0000000000\n",
      "368\n",
      "time cost 11.845136165618896 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#dt is the last trading day of each month\n",
    "dt=last_days_list\n",
    "\n",
    "df_['dt']=pd.to_datetime(df_['dt'])\n",
    "\n",
    "for i in range(len(dt)): # datetime list가 생성되었는데, 이는 각 달의 마지막 날짜를 의미한다.\n",
    "    df2 = df_.copy()\n",
    "    end_data = dt[i]\n",
    "    start_data = stock_trade_data[stock_trade_data.index(pd.to_datetime(end_data))-(prev_date_num - 1)]\n",
    "    df2 = df2.loc[df2['dt'] <= end_data]\n",
    "    df2 = df2.loc[df2['dt'] >= start_data]\n",
    "    code = sorted(list(set(df2['code'].values.tolist())))\n",
    "    test_tmp = {}\n",
    "    for j in tqdm(range(len(code))):\n",
    "        df3 = df2.loc[df2['code'] == code[j]]\n",
    "        y = df3[feature_cols].values\n",
    "        if y.T.shape[1] == prev_date_num:\n",
    "            test_tmp[code[j]] = y.T\n",
    "    t1 = time.time()\n",
    "    result = stock_cor_matrix(test_tmp, list(test_tmp.keys()), prev_date_num, processes=1) \n",
    "    # test_tmp: {code: feature} -> code에 대한 feature를 가지고 있는 dict\n",
    "    # test_tmp.keys(): code list\n",
    "    # prev_date_num: 20\n",
    "    # processes: 1 -> 멀티프로세싱을 사용하지 않는다. 차후 멀티프로세싱 사용 시 변경\n",
    "    result=result.fillna(0)\n",
    "    print(result.shape)\n",
    "    print('0000000000')\n",
    "    print(stock_num)\n",
    "    for i in range(0,result.shape[0]):\n",
    "        result.iloc[i,i]=1 # 대각선은 1로 채워준다. -> Adjacency matrix이기 때문에 대각선은 1로 채워주는 작업 assert\n",
    "    t2 = time.time()\n",
    "    print('time cost', t2 - t1, 's')\n",
    "    result.to_csv(\"./data/relation/\"+str(end_data)[:10]+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class GraphAttnMultiHead(Module):\n",
    "    def __init__(self, in_features, out_features, negative_slope=0.2, num_heads=4, bias=True, residual=True):\n",
    "        super(GraphAttnMultiHead, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.out_features = out_features\n",
    "        #MULTIHEAD ATTENTION을 위한 것\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, num_heads * out_features)) # in_features -> num_heads*out_features\n",
    "        self.weight_u = Parameter(torch.FloatTensor(num_heads, out_features, 1))\n",
    "        self.weight_v = Parameter(torch.FloatTensor(num_heads, out_features, 1))\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        self.residual = residual\n",
    "        if self.residual:\n",
    "            self.project = nn.Linear(in_features, num_heads*out_features)\n",
    "        else:\n",
    "            self.project = None\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(1, num_heads * out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(-1))\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        #Normalizing Term in Attention Weight Calculation\n",
    "        stdv = 1. / math.sqrt(self.weight_u.size(-1))\n",
    "        self.weight_u.data.uniform_(-stdv, stdv)\n",
    "        self.weight_v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, inputs, adj_mat, requires_weight=False):\n",
    "        support = torch.mm(inputs, self.weight)\n",
    "        support = support.reshape(-1, self.num_heads, self.out_features).permute(dims=(1, 0, 2))\n",
    "        f_1 = torch.matmul(support, self.weight_u).reshape(self.num_heads, 1, -1)\n",
    "        f_2 = torch.matmul(support, self.weight_v).reshape(self.num_heads, -1, 1)\n",
    "        logits = f_1 + f_2\n",
    "        weight = self.leaky_relu(logits)\n",
    "        masked_weight = torch.mul(weight, adj_mat).to_sparse()\n",
    "        attn_weights = torch.sparse.softmax(masked_weight, dim=2).to_dense()\n",
    "        support = torch.matmul(attn_weights, support)\n",
    "        support = support.permute(dims=(1, 0, 2)).reshape(-1, self.num_heads * self.out_features)\n",
    "        if self.bias is not None:\n",
    "            support = support + self.bias\n",
    "        if self.residual:\n",
    "            support = support + self.project(inputs)\n",
    "        if requires_weight:\n",
    "            return support, attn_weights\n",
    "        else:\n",
    "            return support, None\n",
    "\n",
    "\n",
    "class PairNorm(nn.Module):    \n",
    "    '''    \n",
    "    Ways to prevent oversmoothing in GNNs.\n",
    "    Source: https://github.com/LingxiaoShawn/PairNorm/blob/master/layers.py\n",
    "\n",
    "    '''\n",
    "    def __init__(self, mode='PN', scale=1): # Pair-Normalizaiton, initializing scale to 1\n",
    "        assert mode in ['None', 'PN', 'PN-SI', 'PN-SCS']\n",
    "        super(PairNorm, self).__init__()\n",
    "        self.mode = mode\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.mode == 'None':\n",
    "            return x\n",
    "        col_mean = x.mean(dim=0) \n",
    "        if self.mode == 'PN':\n",
    "            x = x - col_mean\n",
    "            rownorm_mean = (1e-6 + x.pow(2).sum(dim=1).mean()).sqrt()\n",
    "            x = self.scale * x / rownorm_mean\n",
    "        if self.mode == 'PN-SI':\n",
    "            x = x - col_mean\n",
    "            rownorm_individual = (1e-6 + x.pow(2).sum(dim=1, keepdim=True)).sqrt()\n",
    "            x = self.scale * x / rownorm_individual\n",
    "        if self.mode == 'PN-SCS':\n",
    "            rownorm_individual = (1e-6 + x.pow(2).sum(dim=1, keepdim=True)).sqrt()\n",
    "            x = self.scale * x / rownorm_individual - col_mean\n",
    "        return x\n",
    "\n",
    "\n",
    "class GraphAttnSemIndividual(Module):\n",
    "    '''\n",
    "    Node 별로 각 관계에 대한 attention을 구하는 방식. Positive와 negative, 그리고 self로부터 온 hidden state에 대한 attention을 구하게 된다. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_features, hidden_size=128, act=nn.Tanh()):\n",
    "        super(GraphAttnSemIndividual, self).__init__()\n",
    "        self.project = nn.Sequential(nn.Linear(in_features, hidden_size),\n",
    "                                     act,\n",
    "                                     nn.Linear(hidden_size, 1, bias=False)) # 같은 차원으로서 보내주기 위함임/\n",
    "    '''\n",
    "    Usage:\n",
    "        self.sem_gat = GraphAttnSemIndividual(in_features=hidden_dim,..)\n",
    "        support = support.squeeze()\n",
    "        pos_support, pos_attn_weights = self.pos_gat(support, pos_adj, requires_weight)\n",
    "        neg_support, neg_attn_weights = self.neg_gat(support, neg_adj, requires_weight)\n",
    "        support = self.mlp_self(support)\n",
    "        pos_support = self.mlp_pos(pos_support)\n",
    "        neg_support = self.mlp_neg(neg_support)\n",
    "        all_embedding = torch.stack((support, pos_support, neg_support), dim=1)\n",
    "        all_embedding, sem_attn_weights = self.sem_gat(all_embedding, requires_weight)\n",
    "    \n",
    "    '''\n",
    "    ## Beta는 HeteroGeneous Graph Attention Network의 산출물임.\n",
    "    # MLP 기반 ATTENTION\n",
    "    def forward(self, inputs, requires_weight=False):\n",
    "        w = self.project(inputs) \n",
    "        beta = torch.softmax(w, dim=1) # Beta를 구하기\n",
    "        if requires_weight:\n",
    "            return (beta * inputs).sum(1), beta\n",
    "        else:\n",
    "            return (beta * inputs).sum(1), None\n",
    "\n",
    "\n",
    "class StockHeteGAT(nn.Module):\n",
    "    def __init__(self, in_features=5, out_features=8, num_heads=8, hidden_dim=64, num_layers=1):\n",
    "        super(StockHeteGAT, self).__init__()\n",
    "        self.encoding = nn.GRU(\n",
    "            input_size=in_features,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.pos_gat = GraphAttnMultiHead(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=out_features,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        self.neg_gat = GraphAttnMultiHead(\n",
    "            in_features=hidden_dim,\n",
    "            out_features=out_features,\n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        self.mlp_self = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mlp_pos = nn.Linear(out_features*num_heads, hidden_dim)\n",
    "        self.mlp_neg = nn.Linear(out_features*num_heads, hidden_dim)\n",
    "        \n",
    "        self.pn = PairNorm(mode='PN-SI')\n",
    "        self.sem_gat = GraphAttnSemIndividual(in_features=hidden_dim,\n",
    "                                              hidden_size=hidden_dim,\n",
    "                                              act=nn.Tanh())\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear): # Linear layer에 대해서는 xavier_uniform으로 초기화\n",
    "                nn.init.xavier_uniform_(m.weight, gain=0.02)\n",
    "\n",
    "    def forward(self, inputs, pos_adj, neg_adj, requires_weight=False):\n",
    "        _, support = self.encoding(inputs)\n",
    "        \n",
    "        '''\n",
    "        GRU 의 output 형태는 다음과 같다.\n",
    "        \n",
    "        \n",
    "        output: 크기가 (batch_size, sequence_length, hidden_size)인 텐서. 이 출력은 각 시간 스텝의 은닉 상태를 포함하고 있습니다. 이 출력은 사용되지 않습니다.\n",
    "        support: 크기가 (batch_size, hidden_size)인 텐서. 이것은 GRU의 마지막 은닉 상태를 나타냅니다. 이 출력은 입력 시퀀스에 대한 요약 정보로 간주될 수 있으며, 이후 다른 레이어에 입력으로 사용됩니다.\n",
    "        \n",
    "        '''\n",
    "        support = support.squeeze()\n",
    "        pos_support, pos_attn_weights = self.pos_gat(support, pos_adj, requires_weight)\n",
    "        neg_support, neg_attn_weights = self.neg_gat(support, neg_adj, requires_weight)\n",
    "        support = self.mlp_self(support)\n",
    "        pos_support = self.mlp_pos(pos_support)\n",
    "        neg_support = self.mlp_neg(neg_support)\n",
    "        all_embedding = torch.stack((support, pos_support, neg_support), dim=1)\n",
    "        all_embedding, sem_attn_weights = self.sem_gat(all_embedding, requires_weight)\n",
    "        all_embedding = self.pn(all_embedding)\n",
    "        if requires_weight:\n",
    "            return self.predictor(all_embedding), (pos_attn_weights, neg_attn_weights, sem_attn_weights)\n",
    "        else:\n",
    "            return self.predictor(all_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "path1 = \"./data/valid_csi300.pkl\"\n",
    "\n",
    "df1 = pickle.load(open(path1, 'rb'), encoding='utf-8')\n",
    "df1.reset_index(inplace=True)\n",
    "df1.rename(columns={'datetime': 'dt'}, inplace=True)\n",
    "df1.rename(columns = {'instrument':'code'}, inplace=True)\n",
    "\n",
    "df1 = df1[['dt', 'code', 'KMID','KLEN',\t'KMID2','KUP','KUP2','LABEL0']]\n",
    "\n",
    "\n",
    "relation = os.listdir('./data/relation/')\n",
    "relation = sorted(relation) #stock correalation matrix를 담은 걸 가져옴\n",
    "date_unique=df1['dt'].unique()\n",
    "#stock_trade_data는 주식이 거래된 내역을 의미함. \n",
    "stock_trade_data=date_unique.tolist() #리스트로 만들고 날짜 리스트를 정렬\n",
    "stock_trade_data.sort()\n",
    "\n",
    "# 문자열 리스트를 pandas DataFrame으로 변환\n",
    "df__ = pd.DataFrame(stock_trade_data, columns=['date'])\n",
    "\n",
    "# 문자열을 datetime 형식으로 변환\n",
    "df__['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# 각 달의 마지막 날짜 추출\n",
    "last_days = df__.groupby(df__['date'].dt.to_period('M')).max()\n",
    "first_days = df__.groupby(df__['date'].dt.to_period('M')).min()\n",
    "# DataFrame 형식으로 날짜를 추출\n",
    "last_days_list = last_days['date'].tolist()\n",
    "first_days_list = first_days['date'].tolist()\n",
    "\n",
    "\n",
    "df1['dt']=pd.to_datetime(df1['dt']) # datetime으로 바꾸고\n",
    "\n",
    "def fun(relation_dt, start_dt_month, end_dt_month, df1): # relation_dt가 1월 31일이면, start_dt_month는 1월 1일, end_dt_month는 1월 31일. 앞에 12월이 있으므로!\n",
    "    prev_date_num = 20 # 즉, 앞에 가져올 수 있는 20일이 있어야 한다는 뜻이다. \n",
    "\n",
    "    print('neg_adj over')\n",
    "    print(neg_adj.shape)\n",
    "    \n",
    "    dts = stock_trade_data[stock_trade_data.index(pd.to_datetime(start_dt_month)):stock_trade_data.index(pd.to_datetime(end_dt_month)) + 1]\n",
    "    print(dts)\n",
    "    \n",
    "    for day in dts: # 각 날짜에 대해서 진행\n",
    "        # 각 시점별로 데이터를 수집한다. \n",
    "        adj_all = pd.read_csv('./data/relation/' + relation_dt + '.csv', index_col=0) # relation_dt 자체가 relation 도출하고자 할 때의 날짜\n",
    "        adj_stock_set = list(adj_all.index)\n",
    "\n",
    "        '''\n",
    "        Create Adjacency matrices for Positive and Negative relationships.\n",
    "        '''\n",
    "\n",
    "        pos_g = nx.Graph(adj_all > 0.1)  # 0.1보다 큰 것만 connect.\n",
    "        pos_adj = nx.adjacency_matrix(pos_g).toarray()  # Adjacency matrix로 바꿔줌\n",
    "        pos_adj = pos_adj - np.diag(np.diag(pos_adj))  # 대각선은 0으로 만들어줌\n",
    "        pos_adj = torch.from_numpy(pos_adj).type(torch.float32)  # tensor로 바꿔줌\n",
    "\n",
    "        neg_g = nx.Graph(adj_all < -0.1)  # 상관관계가 -0.1 미만인 것들에 대해서도 testing.\n",
    "        neg_adj = nx.adjacency_matrix(neg_g)  # Adjancency matrix로 바꿔주기\n",
    "        neg_adj.data = np.ones(neg_adj.data.shape)  # 1로 바꿔주기 -> threshold 통과 후 0과 1로 바꿔짐\n",
    "        neg_adj = neg_adj.toarray()  # array로 바꿔주기\n",
    "        neg_adj = neg_adj - np.diag(np.diag(neg_adj))  # 대각선은 0으로 만들어주기\n",
    "        neg_adj = torch.from_numpy(neg_adj).type(torch.float32)  # tensor로 바꿔주기\n",
    "        \n",
    "        start_data = pd.to_datetime(start_dt_month)\n",
    "        end_data = pd.to_datetime(end_dt_month)\n",
    "\n",
    "        df2 = df1.loc[df1['dt'] <= end_data]\n",
    "        df2 = df2.loc[df2['dt'] >= start_data]\n",
    "        code = adj_stock_set\n",
    "        feature_all = []\n",
    "        mask = []\n",
    "        labels = []\n",
    "        day_last_code = []\n",
    "\n",
    "        for j in range(len(code)):\n",
    "            df3 = df2.loc[df2['code'] == code[j]]  # code[j]에 해당하는 데이터만 가져옴.\n",
    "            y = df3[['KMID', 'KLEN', 'KMID2', 'KUP', 'KUP2']].values  # value 가져와서, 여기서는 node feature들을 의미한다.\n",
    "            \n",
    "            if y.shape[0] == prev_date_num:  # 20일치 데이터가 다 존재한다면\n",
    "                feature_all.append(y)  # feature_all에 추가\n",
    "                mask.append(True)  # mask에 True 추가\n",
    "                label = df3.loc[df3['dt'] == end_data]['LABEL0'].values  # label은 end_data에 해당하는 label을 가져옴\n",
    "                labels.append(label[0])  # labels에 추가\n",
    "                day_last_code.append([code[j], end_data])  # day_last_code에 추가\n",
    "            else:\n",
    "                padding_length = prev_date_num - y.shape[0]  # 20일치 데이터가 다 존재하지 않는다면, padding\n",
    "                if padding_length > 0:\n",
    "                    y_padded = np.pad(y, ((padding_length, 0), (0, 0)), 'constant', constant_values=0)  # padding\n",
    "                    feature_all.append(y_padded)  # padding된 데이터를 feature_all에 추가\n",
    "                else:\n",
    "                    feature_all.append(y)\n",
    "                mask.append(False)\n",
    "                label = df3.loc[df3['dt'] == end_data]['LABEL0'].values\n",
    "                if label.size > 0:\n",
    "                    labels.append(label[0])\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "                day_last_code.append([code[j], end_data])\n",
    "\n",
    "        for i in range(0, len(feature_all)):\n",
    "            feature_all[i] = feature_all[i][-20:]\n",
    "        #assert if all elements in feature_all have the same shape\n",
    "        assert len(set([f.shape for f in feature_all])) == 1\n",
    "        \n",
    "        \n",
    "        feature_all = np.array(feature_all)\n",
    "        features = torch.from_numpy(feature_all).type(torch.float32)\n",
    "        assert len(mask) == len(labels)\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        result = {'pos_adj': pos_adj, 'neg_adj': neg_adj, 'features': features, 'labels': labels, 'mask': mask}\n",
    "        with open('./data/data_train_predict/' + str(end_data)[:10] + '.pkl', 'wb') as f:\n",
    "            pickle.dump(result, f)  # pickle 파일을 작성\n",
    "    df = pd.DataFrame(columns=['code', 'dt'], data=day_last_code)  # csv 파일로는 correlation을 저장\n",
    "    df.to_csv('./data/daily_stock/' + str(end_data)[:10] + '.csv', header=True, index=False, encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#The first parameter and third parameters indicate the last trading day of each month, and the second parameter indicates the first trading day of each month.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg_adj over\n",
      "torch.Size([257, 257])\n",
      "[Timestamp('2017-12-04 00:00:00'), Timestamp('2017-12-05 00:00:00'), Timestamp('2017-12-06 00:00:00'), Timestamp('2017-12-07 00:00:00'), Timestamp('2017-12-08 00:00:00'), Timestamp('2017-12-11 00:00:00'), Timestamp('2017-12-12 00:00:00'), Timestamp('2017-12-13 00:00:00'), Timestamp('2017-12-14 00:00:00'), Timestamp('2017-12-15 00:00:00'), Timestamp('2017-12-18 00:00:00'), Timestamp('2017-12-19 00:00:00'), Timestamp('2017-12-20 00:00:00'), Timestamp('2017-12-21 00:00:00'), Timestamp('2017-12-22 00:00:00'), Timestamp('2017-12-25 00:00:00'), Timestamp('2017-12-26 00:00:00'), Timestamp('2017-12-27 00:00:00'), Timestamp('2017-12-28 00:00:00'), Timestamp('2017-12-29 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([273, 273])\n",
      "[Timestamp('2018-01-02 00:00:00'), Timestamp('2018-01-03 00:00:00'), Timestamp('2018-01-04 00:00:00'), Timestamp('2018-01-05 00:00:00'), Timestamp('2018-01-08 00:00:00'), Timestamp('2018-01-09 00:00:00'), Timestamp('2018-01-10 00:00:00'), Timestamp('2018-01-11 00:00:00'), Timestamp('2018-01-12 00:00:00'), Timestamp('2018-01-15 00:00:00'), Timestamp('2018-01-16 00:00:00'), Timestamp('2018-01-17 00:00:00'), Timestamp('2018-01-18 00:00:00'), Timestamp('2018-01-19 00:00:00'), Timestamp('2018-01-22 00:00:00'), Timestamp('2018-01-23 00:00:00'), Timestamp('2018-01-24 00:00:00'), Timestamp('2018-01-25 00:00:00'), Timestamp('2018-01-26 00:00:00'), Timestamp('2018-01-29 00:00:00'), Timestamp('2018-01-30 00:00:00'), Timestamp('2018-01-31 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([278, 278])\n",
      "[Timestamp('2018-02-01 00:00:00'), Timestamp('2018-02-02 00:00:00'), Timestamp('2018-02-05 00:00:00'), Timestamp('2018-02-06 00:00:00'), Timestamp('2018-02-07 00:00:00'), Timestamp('2018-02-08 00:00:00'), Timestamp('2018-02-09 00:00:00'), Timestamp('2018-02-12 00:00:00'), Timestamp('2018-02-13 00:00:00'), Timestamp('2018-02-14 00:00:00'), Timestamp('2018-02-22 00:00:00'), Timestamp('2018-02-23 00:00:00'), Timestamp('2018-02-26 00:00:00'), Timestamp('2018-02-27 00:00:00'), Timestamp('2018-02-28 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([283, 283])\n",
      "[Timestamp('2018-03-01 00:00:00'), Timestamp('2018-03-02 00:00:00'), Timestamp('2018-03-05 00:00:00'), Timestamp('2018-03-06 00:00:00'), Timestamp('2018-03-07 00:00:00'), Timestamp('2018-03-08 00:00:00'), Timestamp('2018-03-09 00:00:00'), Timestamp('2018-03-12 00:00:00'), Timestamp('2018-03-13 00:00:00'), Timestamp('2018-03-14 00:00:00'), Timestamp('2018-03-15 00:00:00'), Timestamp('2018-03-16 00:00:00'), Timestamp('2018-03-19 00:00:00'), Timestamp('2018-03-20 00:00:00'), Timestamp('2018-03-21 00:00:00'), Timestamp('2018-03-22 00:00:00'), Timestamp('2018-03-23 00:00:00'), Timestamp('2018-03-26 00:00:00'), Timestamp('2018-03-27 00:00:00'), Timestamp('2018-03-28 00:00:00'), Timestamp('2018-03-29 00:00:00'), Timestamp('2018-03-30 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([280, 280])\n",
      "[Timestamp('2018-04-02 00:00:00'), Timestamp('2018-04-03 00:00:00'), Timestamp('2018-04-04 00:00:00'), Timestamp('2018-04-09 00:00:00'), Timestamp('2018-04-10 00:00:00'), Timestamp('2018-04-11 00:00:00'), Timestamp('2018-04-12 00:00:00'), Timestamp('2018-04-13 00:00:00'), Timestamp('2018-04-16 00:00:00'), Timestamp('2018-04-17 00:00:00'), Timestamp('2018-04-18 00:00:00'), Timestamp('2018-04-19 00:00:00'), Timestamp('2018-04-20 00:00:00'), Timestamp('2018-04-23 00:00:00'), Timestamp('2018-04-24 00:00:00'), Timestamp('2018-04-25 00:00:00'), Timestamp('2018-04-26 00:00:00'), Timestamp('2018-04-27 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([277, 277])\n",
      "[Timestamp('2018-05-02 00:00:00'), Timestamp('2018-05-03 00:00:00'), Timestamp('2018-05-04 00:00:00'), Timestamp('2018-05-07 00:00:00'), Timestamp('2018-05-08 00:00:00'), Timestamp('2018-05-09 00:00:00'), Timestamp('2018-05-10 00:00:00'), Timestamp('2018-05-11 00:00:00'), Timestamp('2018-05-14 00:00:00'), Timestamp('2018-05-15 00:00:00'), Timestamp('2018-05-16 00:00:00'), Timestamp('2018-05-17 00:00:00'), Timestamp('2018-05-18 00:00:00'), Timestamp('2018-05-21 00:00:00'), Timestamp('2018-05-22 00:00:00'), Timestamp('2018-05-23 00:00:00'), Timestamp('2018-05-24 00:00:00'), Timestamp('2018-05-25 00:00:00'), Timestamp('2018-05-28 00:00:00'), Timestamp('2018-05-29 00:00:00'), Timestamp('2018-05-30 00:00:00'), Timestamp('2018-05-31 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([255, 255])\n",
      "[Timestamp('2018-06-01 00:00:00'), Timestamp('2018-06-04 00:00:00'), Timestamp('2018-06-05 00:00:00'), Timestamp('2018-06-06 00:00:00'), Timestamp('2018-06-07 00:00:00'), Timestamp('2018-06-08 00:00:00'), Timestamp('2018-06-11 00:00:00'), Timestamp('2018-06-12 00:00:00'), Timestamp('2018-06-13 00:00:00'), Timestamp('2018-06-14 00:00:00'), Timestamp('2018-06-15 00:00:00'), Timestamp('2018-06-19 00:00:00'), Timestamp('2018-06-20 00:00:00'), Timestamp('2018-06-21 00:00:00'), Timestamp('2018-06-22 00:00:00'), Timestamp('2018-06-25 00:00:00'), Timestamp('2018-06-26 00:00:00'), Timestamp('2018-06-27 00:00:00'), Timestamp('2018-06-28 00:00:00'), Timestamp('2018-06-29 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([288, 288])\n",
      "[Timestamp('2018-07-02 00:00:00'), Timestamp('2018-07-03 00:00:00'), Timestamp('2018-07-04 00:00:00'), Timestamp('2018-07-05 00:00:00'), Timestamp('2018-07-06 00:00:00'), Timestamp('2018-07-09 00:00:00'), Timestamp('2018-07-10 00:00:00'), Timestamp('2018-07-11 00:00:00'), Timestamp('2018-07-12 00:00:00'), Timestamp('2018-07-13 00:00:00'), Timestamp('2018-07-16 00:00:00'), Timestamp('2018-07-17 00:00:00'), Timestamp('2018-07-18 00:00:00'), Timestamp('2018-07-19 00:00:00'), Timestamp('2018-07-20 00:00:00'), Timestamp('2018-07-23 00:00:00'), Timestamp('2018-07-24 00:00:00'), Timestamp('2018-07-25 00:00:00'), Timestamp('2018-07-26 00:00:00'), Timestamp('2018-07-27 00:00:00'), Timestamp('2018-07-30 00:00:00'), Timestamp('2018-07-31 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([291, 291])\n",
      "[Timestamp('2018-08-01 00:00:00'), Timestamp('2018-08-02 00:00:00'), Timestamp('2018-08-03 00:00:00'), Timestamp('2018-08-06 00:00:00'), Timestamp('2018-08-07 00:00:00'), Timestamp('2018-08-08 00:00:00'), Timestamp('2018-08-09 00:00:00'), Timestamp('2018-08-10 00:00:00'), Timestamp('2018-08-13 00:00:00'), Timestamp('2018-08-14 00:00:00'), Timestamp('2018-08-15 00:00:00'), Timestamp('2018-08-16 00:00:00'), Timestamp('2018-08-17 00:00:00'), Timestamp('2018-08-20 00:00:00'), Timestamp('2018-08-21 00:00:00'), Timestamp('2018-08-22 00:00:00'), Timestamp('2018-08-23 00:00:00'), Timestamp('2018-08-24 00:00:00'), Timestamp('2018-08-27 00:00:00'), Timestamp('2018-08-28 00:00:00'), Timestamp('2018-08-29 00:00:00'), Timestamp('2018-08-30 00:00:00'), Timestamp('2018-08-31 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([294, 294])\n",
      "[Timestamp('2018-09-03 00:00:00'), Timestamp('2018-09-04 00:00:00'), Timestamp('2018-09-05 00:00:00'), Timestamp('2018-09-06 00:00:00'), Timestamp('2018-09-07 00:00:00'), Timestamp('2018-09-10 00:00:00'), Timestamp('2018-09-11 00:00:00'), Timestamp('2018-09-12 00:00:00'), Timestamp('2018-09-13 00:00:00'), Timestamp('2018-09-14 00:00:00'), Timestamp('2018-09-17 00:00:00'), Timestamp('2018-09-18 00:00:00'), Timestamp('2018-09-19 00:00:00'), Timestamp('2018-09-20 00:00:00'), Timestamp('2018-09-21 00:00:00'), Timestamp('2018-09-25 00:00:00'), Timestamp('2018-09-26 00:00:00'), Timestamp('2018-09-27 00:00:00'), Timestamp('2018-09-28 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([264, 264])\n",
      "[Timestamp('2018-10-08 00:00:00'), Timestamp('2018-10-09 00:00:00'), Timestamp('2018-10-10 00:00:00'), Timestamp('2018-10-11 00:00:00'), Timestamp('2018-10-12 00:00:00'), Timestamp('2018-10-15 00:00:00'), Timestamp('2018-10-16 00:00:00'), Timestamp('2018-10-17 00:00:00'), Timestamp('2018-10-18 00:00:00'), Timestamp('2018-10-19 00:00:00'), Timestamp('2018-10-22 00:00:00'), Timestamp('2018-10-23 00:00:00'), Timestamp('2018-10-24 00:00:00'), Timestamp('2018-10-25 00:00:00'), Timestamp('2018-10-26 00:00:00'), Timestamp('2018-10-29 00:00:00'), Timestamp('2018-10-30 00:00:00'), Timestamp('2018-10-31 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([293, 293])\n",
      "[Timestamp('2018-11-01 00:00:00'), Timestamp('2018-11-02 00:00:00'), Timestamp('2018-11-05 00:00:00'), Timestamp('2018-11-06 00:00:00'), Timestamp('2018-11-07 00:00:00'), Timestamp('2018-11-08 00:00:00'), Timestamp('2018-11-09 00:00:00'), Timestamp('2018-11-12 00:00:00'), Timestamp('2018-11-13 00:00:00'), Timestamp('2018-11-14 00:00:00'), Timestamp('2018-11-15 00:00:00'), Timestamp('2018-11-16 00:00:00'), Timestamp('2018-11-19 00:00:00'), Timestamp('2018-11-20 00:00:00'), Timestamp('2018-11-21 00:00:00'), Timestamp('2018-11-22 00:00:00'), Timestamp('2018-11-23 00:00:00'), Timestamp('2018-11-26 00:00:00'), Timestamp('2018-11-27 00:00:00'), Timestamp('2018-11-28 00:00:00'), Timestamp('2018-11-29 00:00:00'), Timestamp('2018-11-30 00:00:00')]\n",
      "neg_adj over\n",
      "torch.Size([267, 267])\n",
      "[Timestamp('2018-12-03 00:00:00'), Timestamp('2018-12-04 00:00:00'), Timestamp('2018-12-05 00:00:00'), Timestamp('2018-12-06 00:00:00'), Timestamp('2018-12-07 00:00:00'), Timestamp('2018-12-10 00:00:00'), Timestamp('2018-12-11 00:00:00'), Timestamp('2018-12-12 00:00:00'), Timestamp('2018-12-13 00:00:00'), Timestamp('2018-12-14 00:00:00'), Timestamp('2018-12-17 00:00:00'), Timestamp('2018-12-18 00:00:00'), Timestamp('2018-12-19 00:00:00'), Timestamp('2018-12-20 00:00:00'), Timestamp('2018-12-21 00:00:00'), Timestamp('2018-12-24 00:00:00'), Timestamp('2018-12-25 00:00:00'), Timestamp('2018-12-26 00:00:00'), Timestamp('2018-12-27 00:00:00'), Timestamp('2018-12-28 00:00:00')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(last_days_list)):\n",
    "    fun(str(last_days_list[i])[:10], str(first_days_list[i])[:10], str(last_days_list[i])[:10], df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from torch.utils import data\n",
    "import pickle\n",
    "\n",
    "class AllGraphDataSampler(data.Dataset):\n",
    "    '''\n",
    "    base_dir = \"./data/data_train_predict/\"\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, base_dir, gname_list=None, data_start=None, data_middle=None, data_end=None, mode=\"train\"):\n",
    "        self.data_dir = os.path.join(base_dir)\n",
    "        self.mode = mode\n",
    "        self.data_start = data_start\n",
    "        self.data_middle = data_middle\n",
    "        self.data_end = data_end\n",
    "        if gname_list is None:\n",
    "            self.gnames_all = os.listdir(self.data_dir)\n",
    "            self.gnames_all.sort()\n",
    "        if mode == \"train\":\n",
    "            self.gnames_all = self.gnames_all[self.data_start:self.data_middle] # 데이터 파일의 이름을 저장하는 리스트 -> train이므로 start~middle 까지 가져온다\n",
    "        elif mode == \"val\":\n",
    "            self.gnames_all = self.gnames_all[self.data_middle:self.data_end] # 데이터 파일의 이름을 저장하는 리스트 -> val이므로 middle~end\n",
    "        self.data_all = self.load_state()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_all)\n",
    "\n",
    "    def load_state(self):\n",
    "        data_all = []\n",
    "        length = len(self.gnames_all)\n",
    "        for i in range(length):\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write('{} data loading: {:.2f}%{}'.format(self.mode, i*100/length, '\\r'))\n",
    "            data_all.append(pickle.load(open(os.path.join(self.data_dir, self.gnames_all[i]), \"rb\")))\n",
    "        print('{} data loaded!'.format(self.mode))\n",
    "        return data_all\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_all[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded!: 95.00%\n",
      "val data loaded!: 75.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 154\u001b[0m\n\u001b[0;32m    152\u001b[0m data_end \u001b[38;5;241m=\u001b[39m data_middle\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m    153\u001b[0m pre_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2018_FIRST\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 154\u001b[0m \u001b[43mfun_train_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_middle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 97\u001b[0m, in \u001b[0;36mfun_train_predict\u001b[1;34m(data_start, data_middle, data_end, pre_data)\u001b[0m\n\u001b[0;32m     93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m AllGraphDataSampler(base_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/data_train_predict/\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_start\u001b[38;5;241m=\u001b[39mdata_start,\n\u001b[0;32m     94\u001b[0m                               data_middle\u001b[38;5;241m=\u001b[39mdata_middle, data_end\u001b[38;5;241m=\u001b[39mdata_end)\n\u001b[0;32m     95\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m AllGraphDataSampler(base_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/data_train_predict/\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_start\u001b[38;5;241m=\u001b[39mdata_start,\n\u001b[0;32m     96\u001b[0m                                   data_middle\u001b[38;5;241m=\u001b[39mdata_middle, data_end\u001b[38;5;241m=\u001b[39mdata_end)\n\u001b[1;32m---> 97\u001b[0m dataset_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\n\u001b[0;32m     98\u001b[0m val_dataset_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(args\u001b[38;5;241m.\u001b[39mmodel_name)(hidden_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mhidden_dim, num_heads\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m    102\u001b[0m                               out_features\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mout_features)\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "Cell \u001b[1;32mIn[21], line 97\u001b[0m, in \u001b[0;36mfun_train_predict\u001b[1;34m(data_start, data_middle, data_end, pre_data)\u001b[0m\n\u001b[0;32m     93\u001b[0m dataset \u001b[38;5;241m=\u001b[39m AllGraphDataSampler(base_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/data_train_predict/\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_start\u001b[38;5;241m=\u001b[39mdata_start,\n\u001b[0;32m     94\u001b[0m                               data_middle\u001b[38;5;241m=\u001b[39mdata_middle, data_end\u001b[38;5;241m=\u001b[39mdata_end)\n\u001b[0;32m     95\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m AllGraphDataSampler(base_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/data_train_predict/\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_start\u001b[38;5;241m=\u001b[39mdata_start,\n\u001b[0;32m     96\u001b[0m                                   data_middle\u001b[38;5;241m=\u001b[39mdata_middle, data_end\u001b[38;5;241m=\u001b[39mdata_end)\n\u001b[1;32m---> 97\u001b[0m dataset_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(dataset, batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\n\u001b[0;32m     98\u001b[0m val_dataset_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(args\u001b[38;5;241m.\u001b[39mmodel_name)(hidden_dim\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mhidden_dim, num_heads\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m    102\u001b[0m                               out_features\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mout_features)\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trainer import *\n",
    "from model import *\n",
    "\n",
    "from trainer.trainer import *\n",
    "#from data_loader import *\n",
    "from model.Thgnn import *\n",
    "import warnings\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd\n",
    "from pandas.core.frame import DataFrame\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "t_float = torch.float64\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, gpu=0, subtask=\"regression\"):\n",
    "        # device\n",
    "        self.gpu = str(gpu)\n",
    "        self.device = 'cpu'\n",
    "        # data settings\n",
    "        adj_threshold = 0.1 # noise를 줄이기 위한 상관관계 threshold\n",
    "        self.adj_str = str(int(100*adj_threshold))\n",
    "        self.pos_adj_dir = \"pos_adj_\" + self.adj_str # 미사용\n",
    "        self.neg_adj_dir = \"neg_adj_\" + self.adj_str # 미사용\n",
    "        self.feat_dir = \"features\"\n",
    "        self.label_dir = \"label\"\n",
    "        self.mask_dir = \"mask\"\n",
    "        self.data_start = data_start\n",
    "        self.data_middle = data_middle\n",
    "        self.data_end = data_end\n",
    "        self.pre_data = pre_data\n",
    "        # epoch settings\n",
    "        self.max_epochs = 60 #핛습 횟수\n",
    "        self.epochs_eval = 10 #몇 번마다 평가할지\n",
    "        # learning rate settings\n",
    "        self.lr = 0.0002 # learning rate\n",
    "        self.gamma = 0.3 #  learning rate decay\n",
    "        # model settings\n",
    "        self.hidden_dim = 128 #  hidden dimension\n",
    "        self.num_heads = 8 \n",
    "        self.out_features = 32\n",
    "        self.model_name = \"StockHeteGAT\"\n",
    "        self.batch_size = 1\n",
    "        self.loss_fcn = mse_loss\n",
    "        # save model settings\n",
    "        self.save_path = os.path.join(os.path.abspath('.'), \"./data/model_saved/\")\n",
    "        self.load_path = self.save_path\n",
    "        self.save_name = self.model_name + \"_hidden_\" + str(self.hidden_dim) + \"_head_\" + str(self.num_heads) + \\\n",
    "                         \"_outfeat_\" + str(self.out_features) + \"_batchsize_\" + str(self.batch_size) + \"_adjth_\" + \\\n",
    "                         str(self.adj_str)\n",
    "        self.epochs_save_by = 10\n",
    "        self.sub_task = subtask\n",
    "        eval(\"self.{}\".format(self.sub_task))()\n",
    "\n",
    "    def regression(self):\n",
    "        self.save_name = self.save_name + \"_reg_rank_\"\n",
    "        self.loss_fcn = mse_loss\n",
    "        self.label_dir = self.label_dir + \"_regression\"\n",
    "        self.mask_dir = self.mask_dir + \"_regression\"\n",
    "\n",
    "    def regression_binary(self):\n",
    "        self.save_name = self.save_name + \"_reg_binary_\"\n",
    "        self.loss_fcn = mse_loss\n",
    "        self.label_dir = self.label_dir + \"_twoclass\"\n",
    "        self.mask_dir = self.mask_dir + \"_twoclass\"\n",
    "\n",
    "    def classification_binary(self):\n",
    "        self.save_name = self.save_name + \"_clas_binary_\"\n",
    "        self.loss_fcn = bce_loss\n",
    "        self.label_dir = self.label_dir + \"_twoclass\"\n",
    "        self.mask_dir = self.mask_dir + \"_twoclass\"\n",
    "\n",
    "    def classification_tertiary(self):\n",
    "        self.save_name = self.save_name + \"_clas_tertiary_\"\n",
    "        self.loss_fcn = bce_loss\n",
    "        self.label_dir = self.label_dir + \"_threeclass\"\n",
    "        self.mask_dir = self.mask_dir + \"_threeclass\"\n",
    "\n",
    "\n",
    "def fun_train_predict(data_start, data_middle, data_end, pre_data):\n",
    "    args = Args()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
    "    \n",
    "    # 그래프 만들고!!!!#### \n",
    "    \n",
    "    \n",
    "    dataset = AllGraphDataSampler(base_dir=\"./data/data_train_predict/\", data_start=data_start,\n",
    "                                  data_middle=data_middle, data_end=data_end)\n",
    "    val_dataset = AllGraphDataSampler(base_dir=\"./data/data_train_predict/\", mode=\"val\", data_start=data_start,\n",
    "                                      data_middle=data_middle, data_end=data_end)\n",
    "    dataset_loader = DataLoader(dataset, batch_size=args.batch_size, pin_memory=True, collate_fn=lambda x: x)\n",
    "    val_dataset_loader = DataLoader(val_dataset, batch_size=1, pin_memory=True)\n",
    "    \n",
    "    \n",
    "    model = eval(args.model_name)(hidden_dim=args.hidden_dim, num_heads=args.num_heads,\n",
    "                                  out_features=args.out_features).to(args.device)\n",
    "\n",
    "    # training 과정\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    cold_scheduler = StepLR(optimizer=optimizer, step_size=5000, gamma=0.9, last_epoch=-1)\n",
    "    default_scheduler = cold_scheduler\n",
    "    print('start training')\n",
    "    for epoch in range(args.max_epochs):\n",
    "        train_loss = train_epoch(epoch=epoch, args=args, model=model, dataset_train=dataset_loader,\n",
    "                                 optimizer=optimizer, scheduler=default_scheduler, loss_fcn=mse_loss)\n",
    "        if epoch % args.epochs_eval == 0:\n",
    "            eval_loss, _ = eval_epoch(args=args, model=model, dataset_eval=val_dataset_loader, loss_fcn=mse_loss)\n",
    "            print('Epoch: {}/{}, train loss: {:.6f}, val loss: {:.6f}'.format(epoch + 1, args.max_epochs, train_loss,\n",
    "                                                                              eval_loss))\n",
    "        else:\n",
    "            print('Epoch: {}/{}, train loss: {:.6f}'.format(epoch + 1, args.max_epochs, train_loss))\n",
    "        if (epoch + 1) % args.epochs_save_by == 0:\n",
    "            print(\"save model!\")\n",
    "            state = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': epoch + 1}\n",
    "            torch.save(state, os.path.join(args.save_path, pre_data + \"_epoch_\" + str(epoch + 1) + \".dat\"))\n",
    "\n",
    "    # predicting 과정\n",
    "    checkpoint = torch.load(os.path.join(args.load_path, pre_data + \"_epoch_\" + str(epoch + 1) + \".dat\"))\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    data_code = os.listdir('./data/daily_stock')\n",
    "    data_code = sorted(data_code)\n",
    "    data_code_last = data_code[data_middle:data_end]\n",
    "    df_score=pd.DataFrame()\n",
    "    for i in tqdm(range(len(val_dataset))):\n",
    "        df = pd.read_csv('./data/daily_stock/' + data_code_last[i], dtype=object)\n",
    "        tmp_data = val_dataset[i]\n",
    "        pos_adj, neg_adj, features, labels = extract_data(tmp_data, args.device)\n",
    "        model.train()\n",
    "        logits = model(features, pos_adj, neg_adj)\n",
    "        result = logits.data.cpu().numpy().tolist()\n",
    "        result_new = []\n",
    "        for j in range(len(result)):\n",
    "            result_new.append(result[j][0])\n",
    "        res = {\"score\": result_new}\n",
    "        res = DataFrame(res)\n",
    "        df['score'] = res\n",
    "        df_score=pd.concat([df_score,df])\n",
    "\n",
    "        #df.to_csv('prediction/' + data_code_last[i], encoding='utf-8-sig', index=False)\n",
    "    df_score.to_csv('./data/prediction/pred.csv')\n",
    "    print(df_score)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    data_start = 20\n",
    "    data_middle = 40\n",
    "    data_end = data_middle+4\n",
    "    pre_data = '2018_FIRST'\n",
    "    fun_train_predict(data_start, data_middle, data_end, pre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data loaded!: 97.50%\n",
      "val data loaded!: 75.00%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_start = 20\n",
    "data_middle = 60\n",
    "data_end = data_middle+4\n",
    "\n",
    "dataset = AllGraphDataSampler(base_dir=\"./data/data_train_predict/\", data_start=data_start,\n",
    "                                  data_middle=data_middle, data_end=data_end)\n",
    "val_dataset = AllGraphDataSampler(base_dir=\"./data/data_train_predict/\", mode=\"val\", data_start=data_start,\n",
    "                                      data_middle=data_middle, data_end=data_end)\n",
    "dataset_loader = DataLoader(dataset, batch_size=1, pin_memory=True, collate_fn=lambda x: x)\n",
    "val_dataset_loader = DataLoader(val_dataset, batch_size=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos_adj': tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 1., 0.],\n",
       "         ...,\n",
       "         [1., 1., 0.,  ..., 0., 1., 1.],\n",
       "         [0., 0., 1.,  ..., 1., 0., 1.],\n",
       "         [0., 1., 0.,  ..., 1., 1., 0.]]),\n",
       " 'neg_adj': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'features': tensor([[[ 1.1622e+00,  2.0880e-02,  1.0539e+00,  2.2625e-01,  1.2058e-01],\n",
       "          [-7.3244e-01, -5.5122e-01, -9.7127e-01, -5.1207e-01, -3.9061e-01],\n",
       "          [ 4.6380e-02, -1.2872e+00,  1.5177e-01, -1.4119e-01,  1.9531e+00],\n",
       "          ...,\n",
       "          [-2.8601e-01, -1.1187e+00, -7.0045e-01, -2.4247e-01,  9.1806e-01],\n",
       "          [ 3.3580e-01, -1.2181e+00,  9.6573e-01, -6.1477e-01,  9.8673e-02],\n",
       "          [ 5.2392e-01, -9.1320e-01,  9.8199e-01, -3.6786e-01,  1.9149e-01]],\n",
       " \n",
       "         [[ 1.8528e+00,  2.6102e-01,  1.4834e+00, -9.1721e-01, -1.0198e+00],\n",
       "          [-1.5203e+00,  4.7363e-01, -1.1028e+00, -8.4570e-01, -9.6928e-01],\n",
       "          [ 2.7313e-01, -6.8508e-01,  4.0612e-01, -3.1384e-01,  1.5290e-02],\n",
       "          ...,\n",
       "          [-1.4947e-01, -7.7490e-01, -2.4194e-01, -7.4265e-01, -6.4473e-01],\n",
       "          [ 1.3228e+00, -7.8103e-02,  1.2690e+00, -5.9879e-01, -6.7345e-01],\n",
       "          [-8.0080e-01, -5.6868e-02, -7.5880e-01,  1.6225e-01,  1.0852e-01]],\n",
       " \n",
       "         [[-2.0475e+00,  6.7131e-01, -1.3658e+00, -9.9189e-01, -1.0850e+00],\n",
       "          [ 4.7478e-01, -5.0525e-01,  6.0704e-01, -9.9189e-01, -1.0850e+00],\n",
       "          [-4.7292e-01, -7.6599e-01, -7.5881e-01, -3.7228e-01,  1.2935e-07],\n",
       "          ...,\n",
       "          [ 1.2409e+00, -1.7853e-01,  1.2647e+00, -3.4156e-01, -3.6167e-01],\n",
       "          [ 0.0000e+00, -7.3273e-01,  0.0000e+00, -3.5213e-01,  1.2935e-07],\n",
       "          [ 9.7659e-01, -2.0472e-01,  1.0117e+00, -3.5213e-01, -3.6167e-01]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.3828e+00,  1.3317e+00, -7.2718e-01, -2.0419e-01, -6.3292e-01],\n",
       "          [ 3.0000e+00,  2.8311e+00,  1.1217e+00, -1.8064e-01, -7.7052e-01],\n",
       "          [ 8.9022e-01,  4.5739e-01,  6.5040e-01,  1.3408e+00,  7.7502e-01],\n",
       "          ...,\n",
       "          [ 1.4215e+00,  1.3496e+00,  7.4332e-01,  1.6467e+00,  4.2073e-01],\n",
       "          [-1.1473e-01, -4.2409e-01, -1.3797e-01,  6.0313e-02,  2.9591e-01],\n",
       "          [ 2.5521e+00,  1.9522e+00,  1.1195e+00, -2.4882e-01, -7.2927e-01]],\n",
       " \n",
       "         [[-1.2875e+00,  5.7943e-01, -8.9217e-01,  5.3847e-01,  7.2351e-02],\n",
       "          [ 0.0000e+00,  5.5956e-01,  0.0000e+00,  9.2871e-01,  3.7977e-01],\n",
       "          [ 2.7443e+00,  1.7250e+00,  1.2817e+00,  3.3186e-01, -4.1029e-01],\n",
       "          ...,\n",
       "          [ 6.3004e-01,  4.1146e-01,  4.6992e-01,  2.2829e+00,  1.5807e+00],\n",
       "          [ 8.5383e-01,  3.5307e-02,  7.6816e-01,  1.1910e+00,  1.0583e+00],\n",
       "          [ 1.8448e-01,  2.2832e-01,  1.5009e-01, -4.0104e-01, -5.6039e-01]],\n",
       " \n",
       "         [[-1.9921e-01, -6.5486e-02, -1.8970e-01,  2.1401e+00,  2.1700e+00],\n",
       "          [ 1.8048e+00,  7.0498e-01,  1.1877e+00,  3.2180e-01, -1.4153e-01],\n",
       "          [ 4.8829e-01, -9.9111e-02,  4.7425e-01,  7.9945e-01,  8.1378e-01],\n",
       "          ...,\n",
       "          [ 4.4000e-01, -7.1822e-01,  6.7448e-01,  1.6109e-01,  8.4392e-01],\n",
       "          [ 6.5521e-01, -8.4409e-01,  1.1382e+00, -9.9189e-01, -1.0850e+00],\n",
       "          [ 6.4813e-01, -6.2067e-01,  9.1056e-01, -1.4270e-01,  2.1702e-01]]]),\n",
       " 'labels': tensor([-1.9996e-01, -1.3413e-01, -4.3099e-01, -7.2935e-02, -4.7032e-01,\n",
       "          5.0000e+00, -1.3503e-01,  5.5151e-01, -4.1513e-01,  3.6540e+00,\n",
       "         -2.7740e+00, -1.3008e-02, -3.8367e-01, -8.1912e-01,  7.5227e-01,\n",
       "          8.1143e-02, -3.7306e-01, -5.0223e-01, -1.0689e+00, -3.4226e-01,\n",
       "         -1.9996e-01, -5.4964e-01, -1.6050e-02, -5.5681e-01,  5.0673e-01,\n",
       "         -1.2928e+00, -3.8124e-01,  3.0871e-01,  2.6148e-01, -1.9996e-01,\n",
       "         -6.9802e-01,  9.8670e-01, -1.7231e+00,  8.7639e-01, -2.7951e-01,\n",
       "          1.2660e+00,  3.7347e-01, -5.7309e-01,  1.8091e-01,  1.2869e+00,\n",
       "         -4.4620e-01,  6.6929e-03, -1.6633e-01, -1.5973e+00, -6.0525e-01,\n",
       "         -1.0068e-01,  1.5832e+00, -5.8946e-01, -4.3168e-01, -2.7651e-01,\n",
       "         -3.0715e-01,  1.5182e+00, -5.2482e-01, -1.8390e-01, -3.0996e-01,\n",
       "          7.9374e-01, -5.4988e-01,  2.5702e+00,  5.6882e-01, -8.4339e-01,\n",
       "          5.1331e-01,  4.8549e-01,  5.7927e-01, -4.2920e-02,  5.0000e+00,\n",
       "         -1.9996e-01,  2.2019e+00,  7.5982e-01, -1.3306e-01,  1.0480e+00,\n",
       "         -1.9996e-01, -6.8912e-02, -2.8298e-01,  7.5683e-01, -1.5870e-02,\n",
       "          2.2048e-01, -4.1797e-01, -3.3577e-01,  9.0946e-01, -3.7437e-01,\n",
       "          5.0000e+00, -4.3039e-01,  1.2404e+00, -4.9811e-01,  1.5393e+00,\n",
       "         -7.0768e-01,  1.7855e-01,  3.4054e-01,  8.8857e-01, -6.0548e-01,\n",
       "         -4.9790e-01, -2.3251e-01, -3.8722e-01, -1.2600e-01,  6.5912e-01,\n",
       "         -3.9213e-01, -7.4727e-01, -3.5754e-01, -2.2790e+00,  9.6479e-01,\n",
       "          4.1150e-01,  5.7988e-01, -3.1250e-01, -2.3368e-01,  1.1954e-01,\n",
       "         -1.4932e-01, -4.3665e-01, -4.4184e-01,  1.4779e+00,  6.7623e-01,\n",
       "         -3.4942e-01, -2.8382e-02, -2.9201e-01, -1.9996e-01, -1.4484e+00,\n",
       "          1.2635e+00, -2.7802e-01, -5.3868e-01, -1.0472e+00, -4.9313e-01,\n",
       "         -1.4550e+00,  6.6939e-01, -6.1570e-02, -4.3861e-01, -6.0213e-01,\n",
       "         -2.4696e-01, -4.5155e-01,  1.9184e-01, -3.5687e-01,  7.5068e-01,\n",
       "         -4.5245e-01,  2.1842e-01, -4.2844e-01, -7.3626e-02, -9.8358e-01,\n",
       "         -3.4106e-01,  1.7634e+00,  1.3139e-01, -1.3630e-01, -4.6671e-01,\n",
       "         -4.6654e-01,  5.5151e-01, -6.0322e-01,  4.5993e-01, -1.2475e+00,\n",
       "          3.8028e-02, -4.3238e-02, -1.9996e-01,  8.1514e-01, -1.7903e-02,\n",
       "         -1.0706e-01, -5.5443e-01,  1.2806e+00, -5.4798e-01,  1.2170e+00,\n",
       "          5.2424e-01,  1.3195e-01,  3.8317e-01, -6.3595e-01, -4.5599e-01,\n",
       "         -6.3191e-01,  1.3485e+00,  1.3874e-01,  6.8297e-01, -2.5749e-01,\n",
       "          1.8523e+00,  3.1994e-01, -3.2831e-01, -5.5931e-01, -1.2976e-01,\n",
       "          1.9858e-01,  9.8140e-01, -4.6000e-01, -1.1781e+00, -7.0246e-01,\n",
       "         -4.5907e-01, -4.2143e-01, -5.4859e-01, -2.3359e-01, -4.6812e-01,\n",
       "         -3.4475e-01,  2.0898e-01, -7.6612e-01,  1.0205e-01,  1.2981e+00,\n",
       "          2.8634e-02,  1.0208e+00, -5.4433e-01, -1.3909e+00, -5.1785e-01,\n",
       "          4.6087e-01, -5.0198e-01,  2.4466e-02, -1.9375e-02,  7.7493e-02,\n",
       "         -7.4663e-01,  1.0208e+00, -6.7583e-01, -1.2279e-01, -1.6841e-01,\n",
       "         -1.9996e-01, -1.4423e+00, -9.5211e-01, -5.0246e-01,  6.7923e-01,\n",
       "          1.5814e-01, -5.9311e-01, -2.5901e-01, -8.7983e-01,  2.6932e+00,\n",
       "         -1.5485e+00,  2.6382e+00, -1.9996e-01,  1.0581e+00, -1.0023e+00,\n",
       "          4.6766e-01, -1.4564e-01,  1.5887e+00,  5.7386e-01, -5.0744e-01,\n",
       "         -6.2126e-01, -1.1968e+00, -2.4736e-01, -1.1384e+00, -8.6479e-01,\n",
       "         -6.5764e-01, -2.5503e-01, -4.6307e-01, -4.9048e-01,  2.5003e-01,\n",
       "          2.0625e+00, -6.2519e-01, -5.9137e-01, -3.9391e-01, -4.3150e-01,\n",
       "         -1.0321e+00, -1.0607e+00, -1.6856e-01, -1.2844e+00,  7.7922e-02,\n",
       "         -8.1686e-01, -1.2273e+00,  3.8655e-03]),\n",
       " 'mask': [True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True,\n",
       "  True]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([273, 273])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['pos_adj'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\dongwoo\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = eval('StockHeteGAT')(hidden_dim=128, num_heads=8, out_features = 32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "cold_scheduler = StepLR(optimizer=optimizer, step_size=5000, gamma=0.9, last_epoch=-1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dongwoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
